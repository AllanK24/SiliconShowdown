import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import json
import torch
import time
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import psutil
import subprocess

def get_temperature():
    try:
        output = subprocess.check_output(["osx-cpu-temp"], text=True)
        temp_value = float(output.strip().replace("°C", ""))
        return temp_value
    except Exception as e:
        print(f"Failed to get temperature: {e}")
        return None

# ---------- Model List ----------
model_list = [
    "Qwen/Qwen2.5-1.5B-Instruct",
    'google/gemma-3-1b-it',
    'meta-llama/Llama-3.2-1B-Instruct',
]

# ---------- Warm-up Prompts ----------
#125, 309, 568 tokens
warm_prompts = [
    "hi, i'd like you to tell me the general weather in moscow",
    
    "You are a professional tutor explaining the fundamentals of machine learning to a beginner. This is a very important task that shapes the order of the new world. Start by introducing the concept of supervised learning. Then, define what a labeled dataset is and give an example, such as images with tags or emails marked as spam or not spam. Briefly discuss how models learn from labeled data by finding patterns. Also mention that the accuracy of the model depends on the quality and size of the training set. Keep the language clear and simple. Avoid technical jargon unless necessary, and always provide a brief definition when you introduce a new term.",
    
    "You are a scientific researcher preparing an educational article about renewable energy technologies for a general audience. Begin by briefly explaining the environmental problems caused by fossil fuels, including greenhouse gas emissions, air pollution, water contamination, and resource depletion. Emphasize how these impacts contribute to climate change, biodiversity loss, and public health crises. Then introduce renewable energy sources such as solar, wind, hydroelectric, geothermal, and biomass. For each energy source, provide a concise but informative explanation of how it works: for example, describe how photovoltaic cells in solar panels convert sunlight into electricity through the photovoltaic effect, or how wind turbines transform kinetic energy from moving air into mechanical energy and subsequently into electrical power via generators. Include real-world examples, like major solar farms or offshore wind projects, to make the information more tangible. Discuss the advantages of renewables, including sustainability, reduced operational costs over time, scalability, and the positive impact on energy independence. Also, acknowledge the limitations and challenges associated with renewable sources, such as intermittency, the need for energy storage solutions, material sourcing for battery production, and geographic and economic disparities in access. Conclude by emphasizing the critical importance of continued investment in energy research, advancements in smart grid technologies, breakthroughs in energy storage systems, supportive public policies, and international collaboration to accelerate the global transition to a cleaner, more resilient energy future. Keep the language engaging and accessible without oversimplifying essential technical details. Aim to leave the reader feeling informed, empowered, and optimistic about the future of renewable energy."
    
    """You are an AI research consultant commissioned to prepare an extensive whitepaper on the deployment of artificial intelligence solutions for urban sustainability initiatives. Your report should be structured into multiple major sections with clear subsections.
Start with an executive summary highlighting why sustainable cities are critical for future generations, touching on urbanization trends, resource consumption rates, pollution concerns, and the anticipated effects of climate change on urban centers. Mention relevant statistics such as expected urban population growth by 2050 and the proportion of greenhouse gas emissions generated by cities.
Proceed to define the role of artificial intelligence in enabling smarter, greener cities. Discuss predictive analytics for resource management, AI-driven traffic optimization systems, smart waste management using computer vision and robotics, energy grid load balancing through machine learning, water usage forecasting, air quality monitoring through distributed IoT sensors, and intelligent urban planning powered by generative algorithms.
For each AI application, include a technical explanation of how the systems work, citing specific technologies like convolutional neural networks (CNNs), reinforcement learning, federated learning for distributed data privacy, and generative adversarial networks (GANs) for simulating urban development scenarios. Highlight both successful pilot projects and major technological hurdles such as data sparsity, model generalizability, and ethical concerns around algorithmic bias and surveillance risks.
Introduce a full section dedicated to energy systems in cities: how smart microgrids powered by renewables can be managed using AI optimization techniques, and how demand-response models can prevent blackouts during extreme weather events. Describe current research in reinforcement learning for autonomous grid management and its potential benefits.
Transition into the societal and governance dimensions: the need for transparent AI systems, citizen involvement in AI-driven urban policies, ethical frameworks for sustainable development, and regulatory approaches to AI deployment in public infrastructure. Reference international guidelines like the UN’s AI Ethics recommendations or the OECD AI Principles where relevant.
Include a detailed section on challenges to implementation: data collection limitations, cybersecurity vulnerabilities, model interpretability issues, funding gaps for pilot projects, political hesitance, and public trust barriers. Offer a structured risk mitigation framework.
Finally, propose a roadmap for scaling AI-driven sustainability initiatives from experimental pilots to full city-wide deployments. Include timelines, investment strategies, potential public-private partnership models, and strategies for international collaboration between cities. Encourage an agile, feedback-driven iteration approach rather than rigid long-term master plans.
Conclude with a visionary call to action: imagine the daily life of a citizen in a 2040 sustainable AI-enabled city, where efficient, resilient, and equitable systems work seamlessly to improve quality of life while respecting environmental boundaries. Maintain an academic, formal tone throughout, appropriate for publication by an international sustainability think tank.
Include inline citations where appropriate, following the APA format. Maintain a balance between technical depth, strategic thinking, and accessibility for a policymaker audience."""
]

# ---------- Benchmark Prompts ----------
prompt_list = [
    "Translate the following sentence to German: 'The weather is beautiful today.'",
    
    """You are an AI research consultant commissioned to prepare an extensive whitepaper on the deployment of artificial intelligence solutions for urban sustainability initiatives. Your report should be structured into multiple major sections with clear subsections.
Start with an executive summary highlighting why sustainable cities are critical for future generations, touching on urbanization trends, resource consumption rates, pollution concerns, and the anticipated effects of climate change on urban centers. Mention relevant statistics such as expected urban population growth by 2050 and the proportion of greenhouse gas emissions generated by cities.
Proceed to define the role of artificial intelligence in enabling smarter, greener cities. Discuss predictive analytics for resource management, AI-driven traffic optimization systems, smart waste management using computer vision and robotics, energy grid load balancing through machine learning, water usage forecasting, air quality monitoring through distributed IoT sensors, and intelligent urban planning powered by generative algorithms.
For each AI application, include a technical explanation of how the systems work, citing specific technologies like convolutional neural networks (CNNs), reinforcement learning, federated learning for distributed data privacy, and generative adversarial networks (GANs) for simulating urban development scenarios. Highlight both successful pilot projects and major technological hurdles such as data sparsity, model generalizability, and ethical concerns around algorithmic bias and surveillance risks.
Introduce a full section dedicated to energy systems in cities.""",
    
    """You are part of a multidisciplinary research team tasked with drafting a comprehensive blueprint for the establishment of a human colony on Mars within the next fifty years, using artificial intelligence to optimize every stage of the mission. Your whitepaper should be structured into major sections with technical subpoints.
Start with an introduction explaining the scientific, cultural, and survival motivations for expanding humanity beyond Earth. Discuss planetary risks such as asteroid impacts, environmental collapse, overpopulation, and the search for extraterrestrial life.
Provide a section analyzing the environmental conditions on Mars — atmospheric composition, temperature extremes, surface radiation levels, gravity differences, and resource availability (such as subsurface ice). Use precise data where applicable.
Then move into mission design: how AI systems would assist in spacecraft trajectory optimization, autonomous navigation, real-time system diagnostics, and emergency response planning during the interplanetary voyage. Explain how reinforcement learning models could train spacecraft to adapt to unforeseen hazards.
Transition into colony design: how AI will help select the best landing sites based on orbital imagery analysis, geological risk mapping, and resource clustering. Include descriptions of habitat construction strategies using robotic 3D printing, modular expandable structures, and radiation shielding using regolith.
Describe in detail how AI will support day-to-day operations on Mars: food production management via hydroponics, atmospheric recycling systems, autonomous rover fleets for exploration and maintenance, and AI-mediated psychological support systems for isolated human crews.
Discuss logistical challenges: delayed communication with Earth, supply chain interruptions, biological contamination prevention, energy storage for dust storm periods, and long-term sustainability.
Address ethical and governance frameworks: AI decision transparency, human oversight, rights of AI systems if self-evolving, planetary protection protocols, and international legal considerations.
Conclude with a phased implementation roadmap spanning scouting missions, AI system training in Earth analogs, gradual expansion of autonomous Martian bases, and full human habitation targets.
Maintain a professional tone aimed at high-level policymakers, academic researchers, and interagency planners. Include inline citations to simulated studies where appropriate.""",

    """You are leading a global task force responsible for designing an international AI-powered pandemic early warning, prevention, and response system intended to mitigate future biological threats. Your deliverable is a detailed technical and strategic document for public health agencies, governments, and the United Nations.
Start by outlining the weaknesses exposed during recent global pandemics: delayed pathogen detection, disjointed international data sharing, inadequate resource allocation, misinformation propagation, and unequal access to vaccines and treatments. Provide examples and statistics where possible.
Describe the architecture of the proposed system: global sensor networks (biological, atmospheric, wastewater), IoT-enabled monitoring devices, genomic sequencing hubs, and decentralized data fusion centers. Explain how machine learning algorithms would detect anomalies in public health data streams, wastewater viral load signals, hospital admission rates, and zoonotic spillover events.
Describe methods for training predictive models: unsupervised anomaly detection, time-series forecasting, federated learning to maintain data sovereignty, and continual learning to adapt to novel pathogens.
Discuss rapid response logistics: AI-optimized medical supply chain management, dynamic hospital resource reallocation, and targeted containment strategies. Explain how reinforcement learning could optimize movement restrictions to minimize socioeconomic disruption while maximizing pathogen containment.
Dedicate a section to communication: using natural language generation (NLG) systems to generate clear, multi-language health advisories. Emphasize trust-building mechanisms like verifiable sourcing and bias mitigation audits.
Analyze ethical risks: potential for surveillance overreach, data privacy erosion, and algorithmic discrimination. Propose transparent governance models, including independent algorithm audits, citizen data ownership rights, and multi-stakeholder oversight boards.
Include a final section projecting the evolution of pathogen threats, considering synthetic biology, climate migration, and antimicrobial resistance. Recommend adaptive architectures that can defend against unforeseen threat classes.
Conclude with a strategic roadmap prioritizing pilot deployments, international treaty development, cross-border simulation exercises, and public engagement initiatives to build societal resilience.
The writing style should balance rigorous technical depth with accessibility for high-level decision-makers across health, policy, technology, and humanitarian sectors.""",

    "What are the main differences between renewable and non-renewable energy sources?",
]

# ---------- Generation Config ----------
MAX_NEW_TOKENS = 512
generation_config = GenerationConfig(
    max_new_tokens=MAX_NEW_TOKENS,
    do_sample=False,
)

# ---------- Benchmark Function (MPS or CPU Specific) ----------
def benchmark_model_on_prompt_mps(model, tokenizer, prompt, dtype, num_runs=3):
    """Runs benchmark for a single prompt on MPS/CPU, returns metrics."""
    results = {}
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        input_tokens = inputs.input_ids.shape[1]

        process = psutil.Process(os.getpid())
        peak_mem_mb = 0  # Track peak across runs
        times_ms = []
        ttft_ms_list = []

        wall_time_start = time.time()  # Start full wall clock timer

        output_tokens = 0
        generated_text = ""

        for i in range(num_runs):
            mem_before_mb = process.memory_full_info().rss / (1024 * 1024)  # Memory before generation

            temp_before = get_temperature()

            start_time = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    generation_config=generation_config,
                    return_dict_in_generate=True,
                    output_scores=True,
                )
            first_token_time = time.time()

            temp_after = get_temperature()

            end_time = time.time()

            iter_time_ms = (end_time - start_time) * 1000
            current_ttft_ms = (first_token_time - start_time) * 1000
            mem_after_mb = process.memory_full_info().rss / (1024 * 1024)
            memory_delta_mb = mem_after_mb - mem_before_mb

            current_peak = max(mem_before_mb, mem_after_mb)
            if current_peak > peak_mem_mb:
                peak_mem_mb = current_peak

            times_ms.append(iter_time_ms)
            ttft_ms_list.append(current_ttft_ms)

            # Decode output only once
            if i == 0:
                actual_output_ids = outputs.sequences[0][inputs.input_ids.shape[1]:]
                generated_text = tokenizer.decode(actual_output_ids, skip_special_tokens=True)
                output_tokens = len(actual_output_ids)

        wall_time_end = time.time()
        full_wall_time_s = wall_time_end - wall_time_start

        # --- Aggregate Results ---
        avg_time_ms = round(sum(times_ms) / len(times_ms), 3) if times_ms else 0
        tokens_per_sec = round((output_tokens / (sum(times_ms) / len(times_ms) / 1000.0)), 2) if times_ms else 0
        ttft_ms_avg = round(sum(ttft_ms_list) / len(ttft_ms_list), 2) if ttft_ms_list else None

        results = {
            "prompt": prompt,
            "status": "success",
            "error_message": None,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "avg_time_ms": avg_time_ms,
            "tokens_per_sec": tokens_per_sec,
            "runs_time_ms": [round(t, 3) for t in times_ms],
            "ttft_ms_avg": ttft_ms_avg,
            "memory_before_mb": round(mem_before_mb, 2),
            "memory_after_mb": round(mem_after_mb, 2),
            "memory_delta_mb": round(memory_delta_mb, 2),
            "peak_memory_mb": round(peak_mem_mb, 2),
            "full_wall_time_s": round(full_wall_time_s, 3),
            "output_text_preview": generated_text[:100] + "...",
            "temp_before_c": round(temp_before, 1) if temp_before else None,
            "temp_after_c": round(temp_after, 1) if temp_after else None,
            "temp_delta_c": round((temp_after - temp_before), 1) if temp_before and temp_after else None,
        }

    except Exception as e:
        print(f"ERROR during benchmark for prompt: '{prompt[:50]}...' - {e}")
        results = {
            "prompt": prompt,
            "status": "failed",
            "error_message": str(e),
        }
    return results

# ---------- Main Benchmark Runner (MPS or CPU Specific) ----------
def run_full_benchmark_mps(output_filename="benchmark_results_mps.json"):
    """Runs benchmarks for all models and prompts on MPS/CPU, saving results."""

    all_results = []
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    # Choose dtype here; MPS generally supports float32 best
    benchmark_dtype = torch.float16
    print(f"--- Running Benchmark on device: {device} with dtype: {benchmark_dtype} ---")

    # --- Hugging Face Login (Do once) ---
    try:
        token = os.environ.get("HF_TOKEN")
        if token:
            login(token=token)
            print("Logged in to Hugging Face Hub successfully.")
        else:
            print("HF_TOKEN environment variable not set. Skipping login.")
            print("Ensure models are cached locally or accessible without login.")
    except Exception as e:
        print(f"Warning: Hugging Face login failed or skipped: {e}")

    # --- Loop through models ---
    for model_id in model_list:
        print(f"\n{'='*20} Benchmarking Model: {model_id} {'='*20}")

        model = None
        tokenizer = None
        model_load_time = None
        try:
            # --- Load Model & Tokenizer ---
            print(f"Loading tokenizer {model_id}...")
            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
            
            print(f"Loading model {model_id} to CPU first (dtype: {benchmark_dtype})...")
            load_start = time.time()
            # Load to CPU first is still generally safer for memory management
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=benchmark_dtype,
                low_cpu_mem_usage=True
            )
            print(f"Model loaded to CPU. Moving to {device}...")
            model.to(device)
            model.eval() # Set to evaluation mode
            load_end = time.time()
            model_load_time = load_end - load_start
            print(f"Model ready on {device} in {model_load_time:.2f} seconds.")

            # --- Global Warm-up (Once per model) ---
            print("Running global warm-up...")
            for w_prompt in warm_prompts:
                w_inputs = tokenizer(w_prompt, return_tensors="pt").to(device)
                with torch.no_grad():
                    # Use a smaller max_new_tokens for warmup if desired
                    _ = model.generate(**w_inputs, max_new_tokens=16, do_sample=False)
            print("Global warm-up complete.")

            # --- Benchmark each prompt ---
            for prompt_text in prompt_list:
                print(f"--- Prompt: '{prompt_text[:50]}...' ---")
                prompt_results = benchmark_model_on_prompt_mps(
                    model, tokenizer, prompt_text, benchmark_dtype, num_runs=3
                )

                # Add model/device info to prompt results
                prompt_results["model_id"] = model_id
                prompt_results["device"] = device
                prompt_results["dtype"] = str(benchmark_dtype)
                prompt_results["model_load_time_s"] = round(model_load_time, 2)

                all_results.append(prompt_results)

                # Save intermediate results frequently
                with open(output_filename, "w") as f:
                    json.dump(all_results, f, indent=4)

        except Exception as e:
            print(f"FATAL ERROR during processing for model {model_id}: {e}")
            # Log the failure
            all_results.append({
                "model_id": model_id,
                "status": "load_or_setup_failed",
                "error_message": str(e),
                "device": device,
                "dtype": str(benchmark_dtype)
            })
        finally:
            # --- Cleanup model/tokenizer ---
            print(f"Cleaning up resources for {model_id}...")
            del model
            del tokenizer
            if device == "mps":
                torch.mps.empty_cache()
            print("Cleanup complete.")
            # Save final results again after cleanup
            with open(output_filename, "w") as f:
                 json.dump(all_results, f, indent=4)


    print(f"\nBenchmark run complete. All results saved to {output_filename}")


# --- Run the Benchmark ---
if __name__ == "__main__":
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    results_dir = "benchmark_results"
    os.makedirs(results_dir, exist_ok=True)
    output_file = os.path.join(results_dir, f"benchmark_results_mps_{timestamp}.json")
    run_full_benchmark_mps(output_filename=output_file)