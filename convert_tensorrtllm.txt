# Gemma3
python3 TensorRT-LLM/examples/models/core/gemma/convert_checkpoint.py --model_dir /home/allank24/Programming/SiliconShowdown/models/gemma/gemma-3-1b-it \
                              --ckpt-type hf \
                              --output-model-dir /home/allank24/Programming/SiliconShowdown/models/gemma/gemma-3-1b-it_tensorr-tllm \
                              --dtype bfloat16

trtllm-build --checkpoint_dir /home/allank24/Programming/SiliconShowdown/models/gemma/gemma-3-1b-it_tensorr-tllm \
            --output_dir /home/allank24/Programming/SiliconShowdown/models/gemma/gemma-3-1b-it_TensorRT-LLM_Engine \
            --gemm_plugin auto \
            --max_batch_size 1 \
            --max_input_len 512 \
            --max_num_tokens 512 \
            --max_seq_len 4096 \
            --logits_dtype float16

# LLAMA 3.2 1B
python3 /home/allank24/Programming/SiliconShowdown/TensorRT-LLM/examples/models/core/llama/convert_checkpoint.py --model_dir models/llama/Llama-3.2-1B-Instruct \
                              --output_dir models/llama/Llama-3.2-1B-Instruct_TensorRT-LLM \
                              --dtype float16

trtllm-build --checkpoint_dir models/llama/Llama-3.2-1B-Instruct_TensorRT-LLM \
            --output_dir models/llama/Llama-3.2-1B-Instruct_TensorRT-LLM_Engine \
            --gemm_plugin auto \
            --max_batch_size 1 \
            --max_input_len 512 \
            --max_num_tokens 512 \
            --max_seq_len 4096 \
            --logits_dtype float16

# Qwen 2.5 1.5B
python3 /home/allank24/Programming/SiliconShowdown/TensorRT-LLM/examples/models/core/qwen/convert_checkpoint.py --model_dir models/qwen/Qwen2.5-1.5B-Instruct \
                              --output_dir models/qwen/Qwen2.5-1.5B-Instruct_TensorRT-LLM \
                              --dtype float16

trtllm-build --checkpoint_dir models/qwen/Qwen2.5-1.5B-Instruct_TensorRT-LLM \
            --output_dir models/qwen/Qwen2.5-1.5B-Instruct_TensorRT-LLM_Engine \
            --gemm_plugin auto \
            --max_batch_size 1 \
            --max_input_len 512 \
            --max_num_tokens 512 \
            --max_seq_len 4096 \
            --logits_dtype float16